# Modulo 3: Adquisición de Datos

Propietario: Carlos Escobar

# Adquisición de Datos

![Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-30_at_23.21.59.png](Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-30_at_23.21.59.png)

Entonces, en lo que hace referencia a esta adquisición de datos es posible realizarla en dos modos fundamentales. El modo clásico se denomina un proceso por lotes o en modo batch, es decir que nos van llegando los datos y una vez que tenemos todo un bloque de datos, se procesa ese conjunto de datos y se va recibiendo el siguiente lote de datos, se procesa y así sucesivamente. Es decir que se van analizando grandes cantidades de datos que ya nos han ido llegando. O la siguiente posibilidad es ir realizar un análisis de los datos en streaming, es decir, ir realizando en modo continuo, en tiempo real, va llegando un dato, se procesa, llega otro dato, se procesa, llega otro dato, se procesa, y así vamos procesando los datos en un tiempo real. 

Para el paradigma MapReduce se ajusta muy bien al procesamiento por lotes ya que tenemo sun conjunto de datos. Entonces diseñamos nuestros procesos Mappers, se realiza una partición de estos bloques, de los lotes para ir realizando un procesamiento de cada uno de estos bloques en cada uno de los procesos Mappers. De modo que así se pueden analizar los distintos bloques en paralelo y cuando tengamos el siguiente lote, podremos procesar el siguiente lote y así de forma continua vamos trabajando con los distintos Batch con los distintos lotes de datos que nos vayan apareciendo.

Entonces cuando nosotros vamos a realizar este procesamiento de datos, podríamos hablar de que podemos trabajar a distintos niveles, podemos procesar en lo que se denominan unos grandes lotes Macro Batch, macro lotes, que podrían llevarnos más de 15 minutos de procesamiento y de gestión de estos datos. También podríamos tener lo que se denominan micro lotes, Micro Batch. En este modo de operación, nosotros estaríamos tratando datos entre 2 minutos y 15 minutos, o sea, sería un batch correspondiente a un periodo más corto, pero significativamente largo, de todos modos. Después you nos acercamos a lo que es el procesamiento en tiempo real y podríamos dividirlo en tres niveles diferentes. Un primer nivel sería un Near Real-Time Decision Support, es decir un soporte a la decisión casi en tiempo real que you estaríamos hablando en una toma de decisiones que estaría rondando pues entre algunos segundos y 2 minutos, es decir, estamos dando una respuesta al sistema en menos de 2 minutos. Más rápido tendríamos lo que sería un procesamiento de eventos, un Near Real Time Event Processing, que you estaríamos hablando de una escala de tiempo entre 100 milisegundos y 2 segundos. Estamos procesando los eventos muchos eventos en un tiempo muy corto para dar una respuesta muy rápida. Y por último you tendríamos lo que sería el Real Time que podríamos decir que estamos trabajando en una escala inferior a los 100 milisegundos, es decir, que estamos procesando bloques de datos a una velocidad muy alta y de esta manera, pues necesitamos darle una respuesta muy rápida al usuario para que pueda actuar de la forma adecuada. Entonces cuando tenemos todo este proceso de adquisición de datos para su colocación en el sistema de archivos, deberemos realizar toda una serie de consideraciones para que este proceso sea correcto.

![Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-30_at_23.32.38.png](Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-30_at_23.32.38.png)

En primer lugar, podemos estar recolectando información que procede de distintas fuentes y en distintos formatos, en distintos tipos de datos y tendremos que realizar una extracción de los datos propiamente de la información que vamos recibiendo. Debemos filtrar esa información para encontrar los datos que son relevantes para el usuario en ese momento. Además debemos realizar un proceso que nos permita validar esos datos, es decir, ver la fiabilidad de esos datos, analizar si son datos correctos. Por otro lado, tenemos que integrar todos los datos que vamos recogiendo en nuestro sistema de archivos para su posterior procesamiento y poder realizar el análisis apropiado de estos datos. A raíz de esto surgen ciertos desafíos:

![Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-30_at_23.37.26.png](Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-30_at_23.37.26.png)

Conforme a esto, se tienen requisitos para el sistema de adquisición:

![Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-30_at_23.40.40.png](Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-30_at_23.40.40.png)

Siguiendo estos requisitos, se deben tener en cuenta algunas consideraciones:

![Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-30_at_23.42.36.png](Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-30_at_23.42.36.png)

# Apache Flume

Es una herramienta que es robusta, simple, estándar, configurable, flexible, distribuida, fiable, extensible y tolerante a fallos para la adquisición de datos, lo que denominamos también "Data Ingestion" y provenientes estos datos de distintos productores y "webservers" y su integración en el sistema de Hadoop. Flume en este entorno gestiona la toma de datos y su integración en el entorno Hadoop de una manera fiable y acoplando las velocidades de los distintos entornos, es decir, los productores van a ir produciendo datos a su propia velocidad y, en cambio, estos datos se podrán integrar en el sistema de almacenamiento a una cierta velocidad dependiendo del rendimiento del sistema de almacenamiento. Entonces, Flume aparece ahí en medio para garantizar un flujo estable y que los datos que se vayan produciendo no se pierdan y se puedan ir integrando en nuestro sistema de almacenamiento de una forma segura y fiable.

![Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_08.11.00.png](Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_08.11.00.png)

Lo que aparece en Flume es que se definen un conjunto de agentes que van a ir recogiendo los eventos procedentes de las distintas aplicaciones y, después, tendremos un proceso recolector, un "data collector" que va a ir tomando los datos que van recogiendo los distintos agentes para transferirlos, finalmente, a lo que es ya el sistema de almacenamiento centralizado, ya sea HDFS o HBase. Entonces, cada uno de estos eventos que va generando la aplicación va a tener dos partes fundamentalmente, primero, un encabezado o "header" y, después, lo que sería ya propiamente la información correspondiente al evento en sí, al dato que se quiera integrar en nuestro sistema de almacenamiento.

![Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_08.12.49.png](Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_08.12.49.png)

Si nos fijamos un poco en cómo es cada uno de estos agentes, podremos ver que cada agente es un "daemon" independiente escrito en Java y dispone de un primer proceso "source" que va a ir recogiendo los eventos que le van llegando, después tiene un "channel", o canal, que va a ir gestionando la llegada de estos eventos provenientes de nuestro fichero de nuestra aplicación, que han pasado por el "source" o fuente y, finalmente, un proceso "sink", o podríamos traducirlo por "sumidero", que irá recogiendo los datos que han pasado por el canal. Es posible que nosotros tengamos múltiples canales para gestionar los datos, es decir, si van llegando muchos datos y procesarlos es más lento, podemos utilizar más de un canal para procesarlo. Y estos canales se pueden utilizar en dos modos diferentes, un modo que sería lo que se denomina en un modo de réplica, es de "replicación", es decir, que los datos que van llegando se procesan simultáneamente por varios canales, se replica el procesamiento de los distintos datos. En cambio, hay otra opción que es, lo que se utiliza es lo que se denomina una "multiplexación", es decir, que los datos que llegan a la fuente o "source" se procesan por un canal o por otro, en función del estado de la ocupación de los distintos canales.

![Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_08.16.06.png](Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_08.16.06.png)

También es posible que tengamos distintas fuentes asociadas a un único canal, en este caso, lo que se denomina es que tenemos un "fan-in", es decir, distintas fuentes nos están sirviendo a un único canal que, después, va a comunicarse con el sumidero, y éste va a ser el que va a transferir los datos al sistema de almacenamiento en el formato correcto.

# Apache Sqoop

![Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_08.21.34.png](Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_08.21.34.png)

Ahí vamos a tener dos procesos, un proceso para importar, en el cual nosotros tenemos nuestra base de datos relacional y vamos a tener que Sqoop va a hacer una petición para pedirle a esa base de datos relacional que le dé la información referente al formato de los datos, y le vamos a pedir los metadatos en el que nos diga cómo están almacenados los datos dentro de esas bases de datos y cómo es cada una de las filas, de las tablas que están formando esas bases de datos relacionales. Una vez que Sqoop obtiene esta información de la base de datos relacional, va a hacer una petición para empezar a hacer el proceso de transferencia de esa base de datos relacional al sistema de almacenamiento de Hadoop. En concreto, se va a hacer una aplicación de un "Map-Only Job", es decir, se va a pedir unas operaciones de "Map". Recordemos el paradigma "MapReduce" que es el paradigma básico de Big Data de Hadoop, pues en este caso se va a hacer unas tareas sólo de "Map-Only", es decir, que se van a ir tomando las filas de las distintas tablas, por los distintos procesos Map que se van a ir generando y cada una de esas filas de las tablas van a ser transformadas en ficheros de nuestro sistema de almacenamiento HDFS, por ejemplo, de Hadoop. De esta manera, se van procesando cada una de las tablas de la base de datos, fila a fila, de una manera paralela, de modo que la transferencia desde una base de datos relacional a Hadoop va a ser relativamente rápida. Este proceso lo realizan los distintos procesos Map, los "Map Jobs" en este caso, y vamos a generar los ficheros para su posterior procesamiento con las herramientas de análisis que dispongamos en nuestro sistema.

![Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_08.57.34.png](Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_08.57.34.png)

En el sentido contrario, también podemos tener el caso en que tengamos unos ficheros en nuestro sistema Hadoop y deseemos devolver estos ficheros hacia lo que es el sistema de bases de datos relacional. En este caso, tendremos que realizar la operación de una forma muy parecida, vamos a tener que, en primer lugar, Sqoop va a tener que pedir a la base de datos relacional, toda la información referente a los metadatos y al formato de las distintas tablas que estén almacenadas en esta base de datos y, una vez que ya ha obtenido la información referente a los datos que van a almacenarse en esa base de datos, vamos a generar los procesos Map que van a ir tomando los distintos ficheros que se encuentren en nuestro sistema de almacenamiento de Big Data, y van a realizar el proceso para transformar estos ficheros en filas de las tablas de la base de datos, de modo que se van a ir integrando en la base de datos estas filas obtenidas a partir de los ficheros que tenemos en nuestro sistema de almacenamiento en Hadoop, en Big Data, ya sea HDFS o HBASE, etcétera.

![Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_09.00.54.png](Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_09.00.54.png)

# Apache Kafka

Es una herramienta para realizar un sistema de mensajes, es lo que se llama un "messaging system". Nosotros tenemos que, cuando hablamos de mensajes, generalmente tendremos un emisor, un "sender", y un receptor o "receiver". Entonces, normalmente, entre un emisor y un receptor se establece una vía de comunicación a través de lo que es una cola, un sistema de colas y se ofrece una cola de mensajes. Así, el "sender", el emisor, emite un mensaje, este mensaje se inserta en una cola y el receptor va tomando los mensajes de esta cola, a medida que van estando disponibles y a medida que va pudiendo procesarlos.

![Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_09.04.07.png](Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_09.04.07.png)

De esta manera, tenemos una comunicación entre emisor y receptor, punto a punto. Pero también es, en muchos casos, más que una comunicación, un sistema de mensajes punto a punto, se utilizan sistemas que se denominan "pub-sub", que es de Publish-Subscribe, que sería un sistema en el que no se basa tanto en una comunicación punto a punto, sino que alguien publica un mensaje y todos aquellos receptores que estén suscritos pueden ver ese mensaje y pueden tomarlo. En este caso, tenemos también el mismo caso que antes, un emisor, una cola y un receptor donde podríamos mandar mensajes, pero en general, lo que tenemos es, no sólo un receptor, sino que disponemos de varios receptores que se han suscrito para ir recibiendo los mensajes que vaya enviando este emisor.

![Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_09.52.40.png](Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_09.52.40.png)

Tenemos entonces que Kafka es una herramienta para gestionar este tipo de sistemas de comunicación "pub-sub". Tenemos distintos productores que van a ir generando información relacionada con distintos temas y se van a ir recogiendo los distintos mensajes, en función de los temas que traten o al tipo de información que contengan y se van a organizar por un concepto de tópicos. A continuación, tenemos unos procesos que denominaremos "brokers", que cada uno de estos brokers va a tomar los mensajes que han ido llegando a cada una de las particiones y tendremos generalmente un broker asociado a cada una de estas particiones, que va a tomar los datos que van llegando a esa partición y los van a ir procesando, cada uno de los brokers gestione una réplica para tener un "backup" de la partición, para que en caso de que se produzca algún problema, se puedan recuperar los mensajes y no se pierdan aquellos mensajes que ya han sido recogidos. Entonces, estos mensajes referentes a una partición en concreto, llegan al broker correspondiente, que el broker lo que va a hacer va a ser, va a ver qué consumidores están registrados y les va a mandar los mensajes correspondientes, que le han ido llegando. De modo que así, el broker gestiona una parte de los mensajes referentes a este tópico y los distintos brokers irán llegando e irán mandando los mensajes de las distintas particiones al consumidor que esté registrado. En el caso habitual, tendremos un broker por cada una de las particiones. Si tenemos más brokers que particiones, va a haber brokers que no van a recibir datos de ninguna partición. Y si tenemos más particiones que brokers, eso suele dar problemas, ya que entonces un broker tiene que gestionar más de una partición y eso va a propiciar que se genere un cierto desbalanceo, con lo cual el procesamiento de los mensajes puede ser menos eficiente.

![Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_09.55.00.png](Modulo%203%20Adquisicio%CC%81n%20de%20Datos%20a64d802b1c704015903577f67f390238/Screen_Shot_2020-10-31_at_09.55.00.png)

En otro caso, podemos tener también que no sea un único el consumidor que está suscrito, sino que tenemos varios consumidores suscritos a un mismo tópico. Entonces, cuando tenemos varios consumidores suscritos a un mismo tópico, lo que vamos a hacer va a ser que, los distintos brokers van a ir mandando los mensajes de forma directa, cada "broker" a un consumidor diferente. De este modo, los consumidores forman lo que se llama un "consumer group", es decir, un grupo de consumidores que en conjunto van a procesar o van a recibir todos los mensajes referentes a ese tópico de las distintas particiones, pero no todos los consumidores van a recibir todos los mensajes, sino que cada uno de los consumidores va a recibir los mensajes correspondientes a una partición